---
title: "Weekly Links #22"
date: 2025-02-22
---

I hope everyone's been having a good February! This week's Weekly is more of a Monthly,
but I hope you enjoy the articles all the same. I had a lot to say about these, so it's
quality of commentary over quantity of posts this time around.


- [**My LLM codegen workflow
  atm**](https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/) (Harper Reed): I'm
  [still](/blog/2023/03/gpt-right-questions) trying to figure out where LLMs fit in my
  workflow. The post emphasizes how fast-moving this stuff is: its guidance "probably
  will not work in 2 weeks, or it will work twice as well. ¯\\\_(ツ)\_/¯" 
  
  Call me boring, but I'd bet that we end up somewhere between the doomers, haters and
  hypebeasts. On one hand, these models are clearly a step change in software development
  similar to how online search engines and StackOverflow changed development back at the
  turn of the millennium. Reasoning models like o1 and o3 are able to be more independent,
  but on the other hand I'm still skeptical of how generative models that are best at
  pushing out code will fit into the design and development of more complex systems. I
  spend less than 40% of my engineering time heads-down writing code and tests. The rest
  of my day-to-day is reacting to how that code reacts to different load patterns from
  users and changing conditions of our underlying system dependencies.
  
  On the other, other hand, I'm thinking about the shape of arguments that those more
  skeptical of AI systems are making feel similar to [Gell-Mann
  Amnesia](https://en.wikipedia.org/wiki/Gell-Mann_amnesia_effect) -- it's easy for me to
  say that while LLMs are excelling at full-stack development they'll never be able to
  build distributed systems, since I feel like my own experience is important in my
  day-to-day work. But who knows? Maybe systems engineers will be the next subdiscipline
  to be blindsided by some new model.
  
- [**Querying Parquet with Millisecond
  Latency**](https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/)
  (Raphael Taylor-Davies and Andrew Lamb): The Apache Parquet format is used through a lot
  of OSS "big data" infrastructure since it compresses extremely well. Once data is in a
  place, though, people often have the gall to want to query it out in a reasonable amount
  of time. This post was a great introduction to the nitty-gritty details of the Parquet
  format along with a walk-through of read-time optimizations, some of which are still not
  implemented in the mainline Parquet reader implementations like Arrow three years later.
- [**Are Efficiency and Horizontal Scalability at
  odds?**](https://buttondown.com/hillelwayne/archive/are-efficiency-and-horizontal-scalability-at-odds/)
  (Hillel Wayne): As always, a thought-provoking post from Hillel. From my own experience
  trawling through some large OSS codebases for distributed systems, it's often surprising
  how obviously inneffecient some parts of the system are that are considered to be off of
  the hot path for the modal use-case.
  
  I agree with Hillel's point at the end of this newsletter that this is probably cultural
  to an extent. Similarly to what I wrote about Parquet above, systems built for scale
  often explicitly trade off interactive latencies as a matter of allocating limited
  development resources even if there isn't a strict technical tradeoff in every case.
  
  This is maybe an interesting place for LLM assisted development! Working with a model to
  optimize some local hot loop in a distributed system reduces the development overhead
  for parts of the system that are more easily unit-testable.
