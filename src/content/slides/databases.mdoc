---
title: "CIS188 Guest Lecture: Databases"
---

In May of 2022 I gave a guest lecture on databases to [CIS 188: DevOps](https://cis1880.org/), a course that I was a TA for in college. It was a great experience to lead a class again and answer questions on databases and life after college. When I gave the presentation I was working on Query Optimization at MongoDB, and I refer to that at points throughout the deck.

---


{% slide type="title" title="Distributed Databases"%}
By Davis Haupt
{% /slide %}

I've been at MongoDB since August. In that time, I've rotated on the SQL engines team, working on a SQL-to-MongoDB compiler for compatibility with existing SQL tooling. I ended up on the Query Optimization team, which is building out a new query optimization engine for the MongoDB query language.

{% slide title="Agenda"%}
### 00: What is a Database?
CIS450 in 5 minutes.
### 01: Data Models & Query Patterns
Comparing Postgres & MongoDB.
### 02: Distributed Databases
Database architecture and scalability.
### 03: Where Next?
The direction the industry is moving in.
{% /slide %}
The topic of this lecture is "Distributed Databases," but I'm not assuming anyone here's taken CIS450 (I never took it myself), so we'll start off closer to the beginning of the story.


{% slide type="top" title="What is a database?" %}
> An organized collection of structured, queryable information *(paraphrased from Oracle.com)*
{% /slide %}

A filesystem might contain tons of structured JSON files, but filesystems don't have the capacity to query for data by anything except for filepath (which is basically an ID). Being able to store information AND get it back out again are key components of a database.

{% slide title="Examples" %}
- MySQL
	- Relational SQL
- Apache Cassandra
	- Column-oriented and distributed
- Redis
	- In-memory datastore & cache layer
{% /slide %}

You've used Redis throughout this class. Its homepage describes it as an "in-memory data store" that can be used "as a database, cache, streaming engine and message broker"

{% slide title="Structured Query Language" %}
- Developed at IBM for System R in 1974
- Declarative
- Relational Lingua Franca
{% /slide %}
Since the 1980s, the dominant database management systems have followed the relational model and over time have standardized on the Structured Query Language, which goes by its acronym SQL (pronounced like the word "sequel"). SQL was designed to be declarative to make it easy for database administrators to access data without worrying about the underlying execution plan. We'll see some examples of SQL in the next few slides.

Relational DBs are also called SQL databases, since today, the query language and the relational model often go hand-in-hand. These terms will be used interchangeably throughout the talk.

{% slide title="The SQL Standard" %}
![xkcd standards](https://imgs.xkcd.com/comics/standards.png)
{% /slide %}
SQL's been standardized since the late 80s, but at that point it had already been defined by its implementations for about a decade. Even though SQL:2016 added native support for JSON, Postgres and other databases built in their own support with their own syntax beforehand. That's even before considering the procedural extensions built on top.

SQL-92 is the last, truly standard SQL specification that has come out.


{% slide title="PostgreSQL" %}
The most popular database that isn't owned & operated by EvilCorp
{% /slide %}
For this talk, we'll be focusing on Postgres's flavor of SQL for simplicity's sake, since it's one of the most popular databases among developers.

{% slide title="!SQL" %}
Because developers love to be contrarian.
{% /slide %}
All SQL databases use the same query language and data model and can be directly compared to each other. 

By comparison, NoSQL encompasses the universe of everything else that’s out there. It’s an incredibly broad label that can refer to any database system that doesn't use SQL as its query language. 

This generally maps on to the concept of "non-relational" databases, but even this is extremely broad, covering everything from column stores like Cassandra, to key/value stores like Redis, and document databases like MongoDB and Fauna.

{% slide title="Our Comparison Today" %}
MongoDB + Postgres
{% /slide %}
All of these databases are optimized for different use cases, but for the sake of time and all of our sanities today, we'll be focusing on MongoDB, a document database, and its query language, MQL. The smaller but still broad category of document database also includes RedisJSON, Couchbase, Fauna, and Amazon DocumentDB, to name a few.

SQL is standard enough that even though I'm showing Postgres queries, the queries may very well run unmodified on a bunch of other SQL databases.

Document databases don't share any query language, so all the Mongo queries will be pretty specific to Mongo.

{% slide title="Databases: What are they good for?" %}
- **C**reate
- **R**etrieve
- **U**pdate
- **D**elete
{% /slide %}
Almost every application that uses a database handles four basic operations commonly called the CRUD operations: Create, Retrieve, Update, and Delete. Some apps are "read heavy" while others are "write heavy". The rates at which different operations are performed is one of the main things that can inform the choice of database and data model.

{% slide title="A data modeling case study" %}
### "Cornell Course Cart" (C3)
Users can:
- View review for a given course
- Add courses to a cart
{% /slide %}
No matter what kind of database you use, you'll have to design a data model for your application. This is a description of the information you're storing in your database, and how different pieces of information are related to each other. Throughout this talk, we'll use the example of a completely hypothetical course review system I'll call the Cornell Course Cart, or C3. 

Users of this site can perform two actions:
1. View reviews for courses.
2. Add courses to their course cart.

{% slide title="Data modeling: relationships" %}

### One-to-one
A user has one cart.

### One-to-many
A course can have multiple sections, but each section belongs to one course.

### Many-to-many

A user's cart can contain many courses, and a course can be in many different users' carts.

{% /slide %}

In any data model, there's three fundamental types of relationship among different pieces of data. On this slide, you can see each relationship type along with an example from the C3 application.


{% slide title="Storing data in SQL" %}
- Data is stored in **tables**
- Each entry is a **row**, and every attribute is a **column**
- Each table has a _fixed_ set of columns known as its **schema**.
Data is modeled primarily through the _relationships_ between tables.

{% /slide %}

Like its name suggests, relational databases put the relations between tables front-and-center.

{% slide title="Building a relational schema" %}

TODO: SQL tables

{% /slide %}

Since tables are flat and must have a fixed set of columns, complex applications build up their data model from many tables that have relationships (called foreign keys) between them. With C3, this is what our schema would look like.
If I want to find out which class a student registered for instead of CIS189, I can follow the section_id field to the corresponding row on the SECTIONS table, and then follow that row's course_id to the COURSES table, where I'll find the name and number for the course.

A key property you'll notice of this database schema is that there's no duplicated data anywhere. The only place you can find a course's title is in the COURSE table. The only place to find which professors teach a class is in each SECTION table.

This is a sign that the database is properly normalized. Eliminating data redundancy makes managing and updating data much easier, since any piece of information only needs to be updated in one row, in one table. compared to a duplicated or denormalized data model. It's the recommended way to store data in a relational database when starting out.

{% slide title="Storing data in MongoDB" %}
- Data stored in ***collections***.
- Each entry is a ***document***, and every attribute is a ***field***.
- Every document in a collection _can_ have a ***flexible schema***.
- Like JSON, documents can contain ***nested fields*** and ***arrays of subobjects***, which are the natural way to express most relationships in the document model.
{% /slide %}

{% slide title="Building a non-relational schema" %}

{% /slide %}

You can see here that one-to-many relationships, which required foreign keys in the relational schema, are expressed naturally as nested sub-documents within an array.

These documents can be easily serialized as JSON hand handled by any client programming language that has a JSON parser.

What is expressed a bit more awkwardly, though, is the course cart. What would happen if a course changed its number, or title?

The user<->course relationship is a many-to-many relationship, and the way I've decided to represent the cart has resulted in some data duplication. This makes it fast and easy to access both a user's cart and a course's info, but it requires updating the information in two places if it ever changes. This is called denormalization, and it's a natural pattern in non-relational databases.


{% slide title="Querying Postgres" %}
#### Key Takeaway: JOINs
```js
select *
from COURSES
  inner join SECTIONS on 
    SECTIONS.course_id = COURSES.id
  inner join REVIEWS on 
	SECTIONS.review_id = REVIEWS.id
where
   COURSES.department = “CIS”
   and COURSES.course_code = “188”
```
{% /slide %}

Here you can see the declarative nature of SQL. Rather than writing for loops where we build up our result set, we declare what fields we want returned to us, and that we will need to join together the COURSE, SECTION and REVIEW tables to retrieve the information we need. JOINs are the critical operation in relational databases. They allow us to construct the logical objects that our application requires without having to pull each table into our application and do the equivalent of a join manually there.

Making the JOIN operation as efficient as possible is an extremely large component of SQL query optimization and database theory. Even so, JOINs remain expensive, and when applications reach a certain scale they will begin to denormalize their data in order to reduce the number of JOINs and be able to process more read queries per second.

{% slide title="Querying MongoDB" %}
```js
db.courses.find({
	department: "CIS", 
	course_number: "188"
});
```
{% /slide %}

With embedded documents, no JOINs necessary here! This MongoDB query will pull out all documents where the department and course number match.

{% slide title="Querying MongoDB" %}

```js
db.courses.find({
	department: "CIS",
	section.number: "005"
});
```

{% /slide %}

Mongo's query language lets you query nested documents very easily. This implicit array traversal is super useful for users, but it's something that's become pretty thorny for us to define precisely on the optimization team. It’s a bit like a join in SQL, and it has some similar optimization issues.

{% slide title="Updating Postgres" %}

```sql
begin;
update SECTIONS 
  set professor = "Campbell Phalen"
  inner join COURSES on 
    SECTIONS.course_id = COURSES.id
  where COURSES.department = "CIS"
    and COURSES.course_number = "188"
    and SECTIONS.professor 
      = "Armaan Tobaccowalla"
update SECTIONS 
  set professor = "Rohan Gupta"
  inner join COURSES on 
    SECTIONS.course_id = COURSES.id
  where COURSES.department = "CIS"
    and COURSES.course_number = "188"
    and SECTIONS.professor = "Peyton Walters"
commit;
```

{% /slide %}

Now let’s see how to update our courses. Armaan and Peyton are, unfortunately, graduating this year, and we want to update our database to reflect some new professors.

Notice here that our two updates are wrapped in a begin...commit block. This is a transaction. Transactions bundle multiple operations into a single all-or-nothing operation. If something goes wrong during either of these updates, the database will be rolled back to the state before either of these operations. After a transaction is committed or aborted, we'll never have a situation where Campbell is teaching two sections and Peyton is teaching another one.

{% slide title="Updating MongoDB" %}
```js
db.courses.updateOne(
  {department: "CIS", course_code: "188"},
  {$set: {
    "sections.$[peyton].professor": 
      "Rohan Gupta",
    "sections.$[armaan].professor": 
      "Campbell Phalen"
  }},
  {arrayFilters: [ {"peyton": {
      "sections.professor": 
        "Peyton Walters"
    }},
    {"armaan": { "sections.professor": 
	  "Armaan Tobaccowalla"
    }},
  ]}
);
```
{% /slide %}

You can see that there wasn't any need for a transaction here, since all of the sections live in the same document, and MongoDB's storage engine enforces atomicity of operations on single documents.

{% slide title="Data normalization for data entry" type="top" %}
- Any piece of information will only exist in one place.
- Applications need to be able to operate over entire
logical entities that are made up from multiple tables.
- Joins help with reading, transactions with writing.
{% /slide %}

The big benefit of normalization in SQL databases is that whenever a piece of information needs to be updated, it only needs to be updated in a single place. When information is changing frequently, this is a really helpful property. For those who subscribe to the "Don't Repeat Yourself" principle, normalization feels really natural. But it's not the whole story.

{% slide title="Data denormalization for data access" type="top" %}
- Information is duplicated in multiple places so it can be read with other data it is commonly accessed with.
- Application must update information in all places on writes.
- At the largest scales, virtually all data schemas are denormalized. Joins are too expensive on huge tables.
{% /slide %}

If you need to optimize for lots of reads, then data denormalization is the way to go.

{% slide title="These days, all the top databases are multi-model." type="point"%}
It’s about which way you’re “with the grain”.
{% /slide %}

Fortunately for you all, I'm not here to shill for my employer. Each of these systems has its benefits and downsides depending on the kind of application you're building and the kind of data you're looking to handle.

These days, all the top databases are multi-model. It's a question of going with the grain and against the grain in whatever system you choose.

{% slide title="Semi-structured Data in Postgres" %}
```sql
SELECT 
  jdoc->'department', 
  jdoc->'course_code'
FROM api 
WHERE jdoc @> '{"department": "CIS"}';
```
{% /slide %}

Since 2014, Postgres has supported the jsonb column type, that can hold unstructured and semi-structured JSON data in regular SQL tables.

{% slide title="Joins and Transactions in MongoDB" %}
```js
db.users.aggregate([
    {$match: {name: "Campbell Phalen"}},
    {$unwind: "$cart.courses"},
    {$lookup: {
      from "courses",
      let:  {
        department: "$department",
        course_number: "$course_number"
      },
      pipeline: [
        {$match: {
          department: "$$department",
          course_number: "$$course_number"
        }
}] }}
])
```
{% /slide %}
I'm not here to shill for my company, but a big misconception about MongoDB is that it can't support JOINs or transactions like a relational database can. It in fact can support these use cases and operations. While the document model makes one-to-many and one-to-one relationships trivial to express and work with, most applications will have some many-to-many relationships that will either require full denormalization or some cross-collection operations to work well. We can see an example with our user's cart. In order to show all the details of courses in the cart, we need to join the user and course collections.

It all comes back to data access patterns and how much you're comfortable going against the grain of your chosen technology. If we can determine that people will be viewing individual sections more often than they'll be viewing their carts, this more expensive operation for getting a user's cart would be acceptable. If people are viewing their carts a lot, we could consider denormalizing the data, and updating course data in the course collection and in each user's cart.

{% slide title="Recap!" %}
### SQL Databases
Data is most naturally stored in normalized, de-duplicated flat tables, regardless of access pattern.

### MongoDB
Data is most naturally stored how objects are accessed by applications.
{% /slide %}
{% slide title="Recap!" %}
### SQL Databases
Rely more heavily on joins to ***read normalized data*** into a usable form by the application.

### MongoDB
Can de-emphasize joins and ***write denormalized data*** how it will eventually be used.
{% /slide %}
  
{% slide title="Distributed Databases" type="title" %}
*That was a lot of Dev, and not much Ops*
{% /slide %}

What does all of this stuff have to do with DevOps and CIS188? A big theme of this class has been all about scaling up applications to run on the cloud while working huge numbers of requests. We've seen all the issues we've encountered when scaling stateless applications, and how technologies like Docker and Kubernetes can make our lives easier. Does any of this apply to stateful databases?

{% slide title="The secret ingredients in the Web Scale sauce" %}
### High Availability
### Horizontal Scaling
{% /slide %}

When we're talking web-scale, we generally are talking about two different properties that we want our system to have. If you've been taking notes this semester, these will sound familiar.
 
{% slide title="High Availability" %}
## Replication
- Store copies of data on multiple nodes in case of node failure
- All queries to the replica set can be answered by a single node
- Low-latency reads with replicas in different regions
{% /slide %}
If one node running our database goes down, our application should not also go down. This is accomplished via data replication. We need to replicate our data to multiple nodes so that a single node failure won't result in data loss.

Traditionally, databases were replicated but not necessarily highly available -- they would require a manual failover operation to a different primary database. These days, with the advent of the Raft consensus algorithm, failovers can be virtually entirely automated and take place in seconds.

Replicated databases can be placed in different geographical regions to enable low-latency reads from many different locations.

{% slide title="Horizontal Scaling" %}
## Sharding
- Split data among multiple nodes to increase throughput & storage capacity.
- Queries may need to touch multiple nodes to get a full answer.
- Low-latency reads and writes with shards in different regions (data locality).
{% /slide %}

As our application gets more and more users, our database should be able to store more data than can fit in a single hard drive bank and handle higher and higher read and write throughputs by adding more nodes that applications can read and write from concurrently. This is generally called data sharding.

Traditionally, horizontal scaling was also handled at the application layer, with the application having some kind of routing table that routed requests to a specific database that had no knowledge of other shards. This meant that there was no way to join information across multiple shards within the database, though, and adding more shards to a database can be extremely costly and time-consuming.

Shards can be placed in different geographic locations for low-latency reads and writes that can remain in the same region.

{% slide title="A note on application-level scaling" %}
- Facebook was sharding their MySQL fleet as early as 2005.
- The goal of all databases is to push shared operational complexity down the stack away from the application
{% /slide %}

{% slide title="A short history of distributed databases" %}
- 2007: Amazon Dynamo Paper
- 2009: MongoDB 1.0 Released
- 2012: Spanner Paper
- 2016: Vitess 2.0 adds cross-shard joins on top of MySQL
- 2017: CockroachDB 1.0 Released
- 2017: Postgres adds native logical replication
{% /slide %}

Notice anything with the dates of these releases?

{% slide title="A short history of distributed databases" %}
- 2007: Amazon Dynamo Paper (NoSQL)
- 2009: MongoDB 1.0 Released (NoSQL)
- 2012: Spanner Paper (SQL-ish)
- 2016: Vitess 2.0 adds cross-shard joins on top of MySQL (SQL)
- 2017: CockroachDB 1.0 Released (SQL)
- 2017: Postgres adds native logical replication (SQL)
{% /slide %}

You can see pretty clearly that Distributed SQL didn’t really come onto the stage until a good 5 or so years after distributed NoSQL.

Now what does SQL and NoSQL have to do with distributed databases? There's a lot of pros and cons when it comes to data modeling and what kind of database you want to use, but for most of the past decade, if you wanted to use a distributed database, your only options were NoSQL databases like Amazon DynamoDB, Apache Cassandra and MongoDB. Why is that the case?

{% slide title="What’s hard about distributed SQL?" %}
- Joins
- Transactions
{% /slide %}

Like we talked about above, JOINs and transactions are much more critical to the use of SQL databases than they are to non-relational DBs. Both of these operations are very tricky to implement on only a single node. 

Trying to reason about data that could live on separate nodes is exponentially more complex than even that.

{% slide title="What’s hard about distributed SQL?" %}
It’s easier to scale/shard denormalized datasets, whether it happens inside or outside of the database.
{% /slide %}

Non-relational DBs, on the other hand, don't have to worry about tables living on different nodes as much. Queries tend to be simpler since denormalized data schemas are optimized to store data how it is accessed.

It's a lot easier to locate a single document that represents a course, with all its associated sections and reviews co-located together on one node, than it is to join together three or four different tables split across however many nodes as a user has shards.

{% slide title="NewSQL" %}
- Cloud Spanner
- CockroachDB
- Vitess (PlanetScale)
- YugabyteDB
- NuoDB
- VoltDB
{% /slide %}

Distributed SQL is a lot more difficult, but the theory and algorithms necessary were worked out over the past decade, and there's now some really cool options available:

Cloud Spanner (technically NoSQL since it uses a different query language, but it is relational)
Vitess (an orchestration layer on top of MySQL)
CockroachDB (Postgres-compatible SQL built in Go)

{% slide title="Where are we going from here?" type="title" %}

To the ~~moon~~ cloud

{% /slide %}

You can see how even setting up a single set of replicas requires a Kubernetes operator to do automatically. Once you bring in multiple shards in multiple regions, handling networking across a database cluster becomes a really difficult problem.

How can we as developers avoid dealing with this complexity? Like other applications, we can move our databases into the cloud and have cloud providers and database vendors manage our deployments for us.

{% slide title="Tuning, operating, indexing, and managing databases is hard" %}
## Database Administrator
A whole profession specializing in DBMS management.
{% /slide %}

On top of all of the complexity of distributed databases, operating any database in production, even on a single node, is extremely difficult. There’s the entire profession of Database Administrator that works on managing databases in production.

{% slide title="Managed Services" %}
- Amazon RDS Dedicated (2009)
- DynamoDB (2012)
- MongoDB Atlas Dedicated (2016)
- CockroachDB Dedicated (2019)
- PlanetScale (2021)
{% /slide %}

Just like application deployment, databases have been moving to the Cloud in order to reduce complexity for developers for the past decade. You can see a few fully managed services in this list here.

Many cloud providers offer managed database services directly, but differently from Kubernetes offerings and other managed services, many database vendors offer their own managed services that build on top of existing public clouds built by AWS, Google and Microsoft. These services, like EnterpriseDB for Postgres, MongoDB Atlas, Cockroach Cloud and Elastic Cloud, claim to have more expertise in operating databases that they were in charge of in developing.

{% slide title="You’re still renting virtual hardware" %}

{% /slide %}

These services as they exist today do leave something to be desired, though. Here’s the MongoDB Atlas dedicated pricing page. You can see there’s still tons of knobs to turn, mostly related to VM sizing. How much RAM, how much storage, how many virtual CPU cores. Even though we’re not physically handling server racks and RAID arrays and CAT6 network cables, we still have to think about our compute resources in these discrete virtual machines that are sold by the cloud providers.

You can see there’s some options for auto-scaling, but it’s not a smooth process. Clusters are scaled up and down between these discrete sizes. The Cloud lets us provision new machines with a snap, but we still have to think about those machines, for the most part.

{% slide title="The Cloud is a leaky abstraction." type="point" %}

{% /slide %}

What I’m really trying to say is many cloud offerings are a leaky abstraction. As developers, we ultimately want to run our application and not worry about what hardware it’s running on. But for lots of Cloud products today, especially databases, this isn’t the case. Companies still employ database administrators to adjust settings in the database to tune performance for each company’s specific workloads.

{% slide title="Amazon DynamoDB" %}
### Cloud-only
No self-hostable version

### Distributed
Data is partitioned & replicated

### Auto-scaling

### Pay for usage
Not compute resources
{% /slide %}

Amazon DynamoDB is, in my opinion a big exception to this. It was built as a distributed system from day 1, unlike traditional SQL databases. It’s not even possible to self-host it on non-AWS hardware. It can scale up and down based on how many reads and writes are being sent to it.

But most importantly, you don’t generally pay for compute resources like vCPUs or RAM on DynamoDB. Instead, you pay directly for usage: how many times you read your data and how many writes you send to the database. Anyone who’s taken NETS212 in this room has probably been burned by this in an AWS bill before. 

This is a big improvement over renting virtual hardware that might sit idle because it can’t scale up and down fast enough to meet the precise demand of an application. AWS is now responsible for all the tuning and scaling of individual database nodes, which you, as a user of DynamoDB, aren’t even aware of.

{% slide title="Serverless billing model" %}
### Cloud-only
No self-hostable version

### Distributed
Data is partitioned & replicated

### Auto-scaling

### Pay for usage
Not compute resources
{% /slide %}

The DynamoDB model is, fundamentally, a serverless model. You never have to think about the logical or physical nodes that your database is actually running on. 

When companies talk about Serverless products, they’re taking advantage of a buzzword, but at the end of the day, the shared thread among different serverless products is the billing model: paying for usage rather than discrete compute resources

{% slide title="Serverless Offerings" %}
- DynamoDB (2012)
- Firebase (2012)
- Fauna (2017)
- MongoDB (Beta 2021)
- CockroachDB (Beta 2021)
- PlanetScale (2021)
{% /slide %}

Here’s another timeline showing a progression of Serverless offerings by some of the databases we’ve discussed today. You can see that while DynamoDB and Firebase have been around since 2012, the space is really starting to heat up in recent years. We’ll see how these products mature over the next year or two, but in my opinion, this is the next step for distributed databases to take.

Applications used to have to shard any data without relying on a database to handle that complexity. Now we have distributed databases that can take care of a ton of distributed systems problems, but lots of operational complexity still remains even when we can use managed cloud services. Ultimately, the industry seems to be moving towards a model where all that complexity is abstracted away from the developer, and a database can be accessed and managed just as easily as a third-party API.

{% slide title="Thanks!" type="title" %}
### Questions?
davis@davishaupt.com
{% /slide %}