---
title: "Culpability"
author: "Bruce Holsinger"
publishYear: 2025
dateCompleted: "2026-02-03"
rating: 4.5
genre: novel
---

Who's at fault in a fatal accident involving a self-driving car? _Culpability_
investigates the abstract morality of the trolley problem while exploring the emotional
fallout among the family who were the lucky survivors of the crash.

The book is firmly rooted in the perspective of the narrator Noah, a successful but
run-of-the-mill lawyer at a Maryland law firm, his brilliant wife Lorelei, a digital
ethicist at the top of her field, and their three teenage children. It's an engaging book
about parenthood and coming of age that has almost as much to say about 21st-century
privilege and the American Dream as it does about ethical algorithms and artificial
intelligence.

I don't think I've read anything that spends as much time on the everyday considerations
and dynamics of marriage or fatherhood. The book was 350 Kindle pages and I was able to
finish it in a few sessions over the course of two days.

In my own media bubble I hear a lot about three-letter acronyms like Artificial
Superintelligence (ASI) and Universal Basic Income (UBI); forces that some argue will move
nations in the coming decades. Holsinger reminds us that we ought to give the same
attention -- if not more -- to how the systems we're integrating into our lives _today_
will affect us and our relationships with the people we care about most.

Digital systems by design and definition are predicated on ones and zeros: absolute truth
and absolute falsehood. Even still, the emergent behavior of modern distributed systems on
top of these simple rules can be so uninterpretable to most human understanding that the
best our existing moral frameworks and legal systems can do is just throw up their
metaphorical hands and shrug their anthropormorphized shoulders.

During the COVID vaccine rollout, a hospital system [put blame on an
algorithm](https://web.archive.org/web/20260101232819/https://www.technologyreview.com/2020/12/21/1015303/stanford-vaccine-algorithm/)
for a blatantly unfair apportionment of vaccine resources that benefited tenured medical
faculty over resident physicians on the front lines. LLMs and diffusion models are trained
to mimic authors and artists. This can be argued to fit in the bucket of copyright fair
use in our legal system while still _feeling_ wrong to many of us. Our system is built
upon the assumption that mortal, moral humans are the progenitor and beneficiaries of
intellectual property. That hasn't been true for a while (the Walt Disney Company
[famously put their finger on the
scale](https://web.archive.org/web/20260212104405/https://web.law.duke.edu/cspd/mickey/),
but Generative AI models that take millions of dollars to train but then can be scaled out
and deployed widely make the limitations of that assumption even more apparent.

_Culpability_ also touches on the limits of our legal system, especially how it intersects
with power and privelege. Noah is a lawyer and the story's backdrop is the the police
investigation of the car crash. Law itself is a field that has attempted to coerce human
experience into discrete judgment for virtually all of recorded history. Machine code has
voltage differentials giving way to ones and zeroes; neural networks have probabilistic
predictions [flattened](https://en.wikipedia.org/wiki/Sigmoid_function) into deterministic
output. The legal system has judges and juries which dispense innocent and guilty
verdicts.

I'm reminded of [_Broken
Volume_](https://web.archive.org/web/20250818154250/https://whitney.org/exhibitions/programmed/art?section=2&subsection=3),
sculpture by Cheyney Thompson I saw at The Whitney in 2019. 3D random walks were generated
by CAD software and cast into concrete. One of the pieces broke in half upon installation,
where it stood for the length of the exhibition. Random walks have the simplest of rules
and can create beautiful shapes, but they don't know about gravity. Algorithms follow
defined rules that are encoded within them. All digital systems are based on our own
incomplete and imperfect assumptions about how the things we create will behave when they
encounter the world that we inhabit. Even when deep learning systems start to pick out
their own patterns in their training data, they're learning from the biases and
imperfections of what we choose to include and what we choose to omit from their training.

People die in a car crash where a self-driving car is involved. Who's at fault? We get
closer and closer to an answer throughout the story, but never quite reach it. Can there
be a singular answer? Do we need there to be one?

